{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, jaccard_similarity_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categories = ['sci.space',] \n",
    "categories = ['sci.space', 'comp.graphics', 'talk.politics.misc', 'rec.sport.hockey', 'comp.sys.mac.hardware'] \n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories = categories, remove = remove )\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, categories = categories, remove = remove )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please get a REAL life.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Phil> Didn't one of the early jet fighters have these?  I also think\n",
      "Phil> the germans did some work on these in WWII.\n",
      "\n",
      "The NACA came up with them before World War II.  NASA is directly\n",
      "descended from the NACA, with space added in.\n",
      "\n",
      "You'll notice that I didn't mention sweep wings even though the\n",
      "X-5, tested at what's now Dryden, had them.  We did steal that one\n",
      "dirctly from the Germans.  The difference is that swept wings don't\n",
      "change their angle of sweep, sweep wings do.  Perhaps the similarity\n",
      "of names has caused some confusion?  747s have swept wings, F-111s\n",
      "have sweep wings.\n",
      "\n",
      "\n",
      "Phil> A lot of this was also done by the military...\n",
      "\n",
      "After NASA aerodynamicists proposed them and NASA test teams\n",
      "demonstrated them.  Richard Whitcomb and R.T. Jones, at Langley\n",
      "Research Center, were giants in the field.\n",
      "\n",
      "Dryden was involved in the flight testing of winglets and area\n",
      "ruling (in the 70s and 50s, respectively).  It's true that we\n",
      "used military aircraft as the testbeds (KC-135 and YF-102) but\n",
      "that had more to do with availability and need than with military\n",
      "involvement.  The YF-102 was completely ours and the KC-135 was\n",
      "bailed to us.  The Air Force, of course, was interested in our\n",
      "results and supportive of our efforts.\n",
      "\n",
      "Dryden flew the first digital fly by wire aircraft in the 70s. No\n",
      "mechnaical or analog backup, to show you how confident we were.\n",
      "General Dynamics decided to make the F-16 flyby-wire when they saw how\n",
      "successful we were.  (Mind you, the Avro Arrow and the X-15 were both\n",
      "fly-by-wire aircraft much earlier, but analog.)\n",
      "\n",
      "Phil> Egad! I'm disagreeing with Mary Shafer!  \n",
      "\n",
      "The NASA habit of acquiring second-hand military aircraft and using\n",
      "them for testbeds can make things kind of confusing.  On the other\n",
      "hand, all those second-hand Navy planes give our test pilots a chance\n",
      "to fold the wings--something most pilots at Edwards Air Force Base\n",
      "can't do.\n",
      "\n",
      "\n",
      "Has anyone heard what game ESPN is showing tonight.  They said they will\n",
      "show whatever game means the most playoff-wise. I would assume this would\n",
      "be the Blues-Tampa game or the Minnesota-Red Wings game...  Anyone heard for\n",
      "sure???\n",
      "\n",
      "\n",
      "Yes, I saw today in 6 o'clock news on KCBS here in San Francisco\n",
      "this statistic quoted. \n",
      "\n",
      "2.2% men had sex with another man.\n",
      "1.3% cinsider themself homosexual.\n",
      "\n",
      "I understand of course that because this statistic goes against\n",
      "common believe and not PC-correct it must be complete BS.\n",
      "\n",
      "Thx\n",
      "\n",
      "vlad\n",
      "\n",
      "Because SCSI works well with removable media, and works well with large\n",
      "capacity devices. The floppy interface you suggest handles the former, but\n",
      "it doesn't have any hooks for dealing with the latter... you'd have to kludge\n",
      "it. Plus, it's extremely low performance. AND, SCSI has gobs of room for\n",
      "expansion compared with a floppy (I can just see it, let's stick a 5.25\",\n",
      "a 3.5\", a tape drive, and a MO drive, all on the floppy interface. The\n",
      "possibilities for unexpected collisions are enormous).\n",
      "Hmm.  $1 billion, lesse... I can probably launch 100 tons to LEO at\n",
      "$200 million, in five years, which gives about 20 tons to the lunar\n",
      "surface one-way.  Say five tons of that is a return vehicle and its\n",
      "fuel, a bigger Mercury or something (might get that as low as two\n",
      "tons), leaving fifteen tons for a one-man habitat and a year's supplies?\n",
      "Gee, with that sort of mass margins I can build the systems off\n",
      "the shelf for about another hundred million tops.  That leaves\n",
      "about $700 million profit.  I like this idea 8-)  Let's see\n",
      "if you guys can push someone to make it happen 8-) 8-)\n",
      "\n",
      "[slightly seriously]\n",
      "Greetings all.\n",
      "\tAccording to a FAQ I read, on 30 July 1992, Joshua C. Jensen posted an \n",
      "article on bitmap manipulation (specifically, scaling and perspective) to the \n",
      "newsgroup rec.games.programmer. (article 7716)\n",
      "\tThe article included source code in Turbo Pascal with inline assembly \n",
      "language.\n",
      "\n",
      "\tI have been unable to find an archive for this newsgroup, or a current \n",
      "email address for Joshua C. Jensen.\n",
      "\tIf anyone has the above details, or a copy of the code, could they \n",
      "please let me know.\tMany thanks.\n",
      "\t\t\t\t\tYours gratefully, etc.  Myles.\n",
      "\n",
      "\n",
      "Well, although this may be an uncommon occurrence (or not) I had a \"bad\"\n",
      "experience with TechWorks.  This past summer I upgraded (increased) the memory in\n",
      "a powerbook and a ci.   When I called to place the order for the PB RAM, I was\n",
      "told by the sales person that they would give me a $50 rebate if I would return\n",
      "the original RAM (which was also TechWorks RAM.)  I followed the instructions for\n",
      "returning the old RAM, expecting to see a credit on my VISA within a few weeks.\n",
      "\n",
      "Well, months went by, and no credit.  After many calls (almost none of which were\n",
      "ever returned - arghhh) I finally found someone who told me \"Why we never\n",
      "received your old chips.\"  I then explained I the procedure that I had\n",
      "followed to return them, to which the person replied \"You mean you sent them\n",
      "US Mail?\" (which I had, per the original sales person's instructions.)  I was\n",
      "told that they their loss of US mail shipments is not uncommon (come on) and that\n",
      "I should have sent the stuff via FedEx, etc.  I reasoned that I had done exactly\n",
      "what I had been told to, but they would not budge, the people I spoke with were\n",
      "absolutely no help.  I sent letters, copies of the original receipts, attempted\n",
      "to trace the package through the US mail, made *many* more phone calls to\n",
      "TechWorks, all to no avail (I wouldn't give-up because I was so disgusted.)\n",
      "Sales/support people, supervisors, there was nothing I could do to pursuade them\n",
      "to \"make it right.\"\n",
      "\n",
      "I finally (in total disgust) wrote a letter to my credit card company, asking\n",
      "them to investigate the problem.   Three weeks later, the credit miraculously\n",
      "appeared on my statement.  I have not (in recent memory) been so disgusted with\n",
      "the service that I received from a company.  In all fairness, they had no way of\n",
      "knowing that was not trying to rip them off, but I went to *such* great lengths\n",
      "to prove to them that this really happened.  Oh well, c'est la vie.  I will never\n",
      "buy another product from them again.\n",
      "Also sprach slegge@kean.ucs.mun.ca ...\n",
      "\n",
      "\n",
      "Two things:\n",
      "\n",
      "1. Didn't the trade deadline pass two weeks ago?\n",
      "\n",
      "2. The FLYERS would never ever EVER give up Lindros, simple as that.\n",
      "\n",
      "Go Flyers, Cup in '94...\n",
      "\n",
      "Mike\n"
     ]
    }
   ],
   "source": [
    "for d in twenty_train.data[:10]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 4 1 3 0 3 1 2]\n"
     ]
    }
   ],
   "source": [
    "print (twenty_train.target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data = vect.transform(twenty_train.data)\n",
    "test_data = vect.transform(twenty_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 ..., 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 81, 84, 85, 86, 88, 90, 91, 92, 93, 99, 100, 103, 104, 107, 112, 113, 117, 118, 124, 125, 127, 143, 145, 152, 153, 195, 232}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.data)\n",
    "res = set(train_data.data)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf = True).fit(train_data)\n",
    "train_data_tfidf = tfidf.transform(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179165\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_data_tfidf.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31002\n"
     ]
    }
   ],
   "source": [
    "print(len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('space', 1086), ('don', 962), ('like', 953), ('just', 896), ('know', 871), ('think', 849), ('people', 833), ('time', 771), ('new', 750), ('10', 692)]\n",
      "31002\n"
     ]
    }
   ],
   "source": [
    "x = list(zip(vect.get_feature_names(), np.ravel(train_data.sum(axis=0))))\n",
    "def SortbyTF(inputStr):\n",
    "    return inputStr[1]\n",
    "x.sort(key=SortbyTF, reverse = True)\n",
    "print (x[:10])\n",
    "print(len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = LogisticRegression(random_state=123332)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_data_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(test_data,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 3, 4, 3, 2, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1877"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 3 4 3 2 1 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print (twenty_test.target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1877"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_score = jaccard_similarity_score(y_true=twenty_test.target, y_pred=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.853489611081513"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.853489611081513"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prediction == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
